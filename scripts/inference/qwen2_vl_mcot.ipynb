{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/xiaoxi/anaconda3/envs/factory/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 8291375616\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from transformers.generation.utils import GenerationMixin\n",
    "from transformers.cache_utils import StaticCache,DynamicCache\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "\n",
    "class CustomGenerationMixin(GenerationMixin):\n",
    "    def _update_model_kwargs_for_generation(\n",
    "        self,\n",
    "        outputs,\n",
    "        model_kwargs,\n",
    "        is_encoder_decoder = False,\n",
    "        num_new_tokens = 1,\n",
    "    ):\n",
    "        # update past_key_values keeping its naming used in model code\n",
    "        cache_name, cache = self._extract_past_from_model_output(outputs)\n",
    "        model_kwargs[cache_name] = cache\n",
    "        if getattr(outputs, \"state\", None) is not None:\n",
    "            model_kwargs[\"state\"] = outputs.state\n",
    "\n",
    "        # update token_type_ids with last value\n",
    "        if \"token_type_ids\" in model_kwargs:\n",
    "            token_type_ids = model_kwargs[\"token_type_ids\"]\n",
    "            model_kwargs[\"token_type_ids\"] = torch.cat([token_type_ids, token_type_ids[:, -1].unsqueeze(-1)], dim=-1)\n",
    "\n",
    "        if not is_encoder_decoder:\n",
    "            # update attention mask\n",
    "            if \"attention_mask\" in model_kwargs:\n",
    "                attention_mask = model_kwargs[\"attention_mask\"]\n",
    "                model_kwargs[\"attention_mask\"] = torch.cat(\n",
    "                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], num_new_tokens))], dim=-1\n",
    "                )\n",
    "        else:\n",
    "            # update decoder attention mask\n",
    "            if \"decoder_attention_mask\" in model_kwargs:\n",
    "                decoder_attention_mask = model_kwargs[\"decoder_attention_mask\"]\n",
    "                model_kwargs[\"decoder_attention_mask\"] = torch.cat(\n",
    "                    [decoder_attention_mask, decoder_attention_mask.new_ones((decoder_attention_mask.shape[0], 1))],\n",
    "                    dim=-1,\n",
    "                )\n",
    "\n",
    "        if model_kwargs.get(\"use_cache\", True):\n",
    "            model_kwargs[\"cache_position\"] = model_kwargs[\"cache_position\"][-1:] + num_new_tokens\n",
    "        else:\n",
    "            past_positions = model_kwargs.pop(\"cache_position\")\n",
    "            new_positions = torch.arange(\n",
    "                past_positions[-1] + 1, past_positions[-1] + num_new_tokens + 1, dtype=past_positions.dtype\n",
    "            ).to(past_positions.device)\n",
    "            model_kwargs[\"cache_position\"] = torch.cat((past_positions, new_positions))\n",
    "        return model_kwargs\n",
    "\n",
    "\n",
    "    def _sample(\n",
    "        self,\n",
    "        input_ids,\n",
    "        logits_processor,\n",
    "        stopping_criteria,\n",
    "        generation_config,\n",
    "        synced_gpus,\n",
    "        streamer,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Generates sequences of token ids for models with a language modeling head using **multinomial sampling** and\n",
    "        can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
    "\n",
    "        Parameters:\n",
    "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "                The sequence used as a prompt for the generation.\n",
    "            logits_processor (`LogitsProcessorList`):\n",
    "                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
    "                used to modify the prediction scores of the language modeling head applied at each generation step.\n",
    "            stopping_criteria (`StoppingCriteriaList`):\n",
    "                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
    "                used to tell if the generation loop should stop.\n",
    "            generation_config ([`~generation.GenerationConfig`]):\n",
    "                The generation configuration to be used as parametrization of the decoding method.\n",
    "            synced_gpus (`bool`):\n",
    "                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n",
    "                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n",
    "            streamer (`BaseStreamer`, *optional*):\n",
    "                Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
    "                through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
    "            model_kwargs:\n",
    "                Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n",
    "                an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
    "\n",
    "        Return:\n",
    "            [`~generation.GenerateDecoderOnlyOutput`], [`~generation.GenerateEncoderDecoderOutput`] or `torch.LongTensor`:\n",
    "            A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
    "            [`~generation.GenerateDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
    "            `return_dict_in_generate=True` or a [`~generation.GenerateEncoderDecoderOutput`] if\n",
    "            `model.config.is_encoder_decoder=True`.\n",
    "        \"\"\"\n",
    "        # init values\n",
    "        pad_token_id = generation_config._pad_token_tensor\n",
    "        output_attentions = generation_config.output_attentions\n",
    "        output_hidden_states = generation_config.output_hidden_states\n",
    "        output_scores = generation_config.output_scores\n",
    "        output_logits = generation_config.output_logits\n",
    "        return_dict_in_generate = generation_config.return_dict_in_generate\n",
    "        max_length = generation_config.max_length\n",
    "        has_eos_stopping_criteria = any(hasattr(criteria, \"eos_token_id\") for criteria in stopping_criteria)\n",
    "        do_sample = generation_config.do_sample\n",
    "\n",
    "        # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        raw_logits = () if (return_dict_in_generate and output_logits) else None\n",
    "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "        if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "            encoder_hidden_states = (\n",
    "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "            )\n",
    "\n",
    "        # keep track of which sequences are already finished\n",
    "        batch_size, cur_len = input_ids.shape\n",
    "        this_peer_finished = False\n",
    "        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n",
    "        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n",
    "\n",
    "        model_forward = self.__call__\n",
    "        if isinstance(model_kwargs.get(\"past_key_values\"), StaticCache):\n",
    "            if self.device.type == \"cuda\":\n",
    "                logger.warning_once(\"Using `torch.compile`.\")\n",
    "                os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n",
    "                model_forward = self.get_compiled_call(generation_config.compile_config)\n",
    "\n",
    "        is_prefill = True\n",
    "        while self._has_unfinished_sequences(\n",
    "            this_peer_finished, synced_gpus, device=input_ids.device, cur_len=cur_len, max_length=max_length\n",
    "        ):\n",
    "            # prepare model inputs\n",
    "            model_inputs, origin_input_ids, origin_pixel_values, origin_image_grid_thw= self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # prepare variable output controls (note: some models won't accept all output controls)\n",
    "            model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n",
    "            model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n",
    "\n",
    "            if is_prefill:\n",
    "                outputs = self(**model_inputs, return_dict=True)\n",
    "                is_prefill = False\n",
    "            else:\n",
    "                outputs = model_forward(**model_inputs, return_dict=True)\n",
    "\n",
    "            num_new_tokens = 1 if origin_input_ids.shape[1]==input_ids.shape[1] else (origin_input_ids.shape[1]-input_ids.shape[1]+1)\n",
    "            # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(\n",
    "                outputs,\n",
    "                model_kwargs,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                num_new_tokens=num_new_tokens,\n",
    "            )\n",
    "            if synced_gpus and this_peer_finished:\n",
    "                continue\n",
    "\n",
    "            # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n",
    "            # (the clone itself is always small)\n",
    "            next_token_logits = outputs.logits[:, -1, :].clone().float()\n",
    "            next_token_logits = next_token_logits.to(input_ids.device)\n",
    "\n",
    "            # pre-process distribution\n",
    "            next_token_scores = logits_processor(input_ids, next_token_logits)\n",
    "\n",
    "            # Store scores, attentions and hidden_states when required\n",
    "            if return_dict_in_generate:\n",
    "                if output_scores:\n",
    "                    scores += (next_token_scores,)\n",
    "                if output_logits:\n",
    "                    raw_logits += (next_token_logits,)\n",
    "                if output_attentions:\n",
    "                    decoder_attentions += (\n",
    "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                    )\n",
    "                    if self.config.is_encoder_decoder:\n",
    "                        cross_attentions += (outputs.cross_attentions,)\n",
    "\n",
    "                if output_hidden_states:\n",
    "                    decoder_hidden_states += (\n",
    "                        (outputs.decoder_hidden_states,)\n",
    "                        if self.config.is_encoder_decoder\n",
    "                        else (outputs.hidden_states,)\n",
    "                    )\n",
    "\n",
    "            # token selection\n",
    "            if do_sample:\n",
    "                probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
    "                # TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\n",
    "                next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "            else:\n",
    "                next_tokens = torch.argmax(next_token_scores, dim=-1)\n",
    "\n",
    "            # finished sentences should have their next token be a padding token\n",
    "            if has_eos_stopping_criteria:\n",
    "                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
    "\n",
    "            # update generated ids, model inputs, and length for next step\n",
    "            input_ids = torch.cat([origin_input_ids, next_tokens[:, None]], dim=-1)\n",
    "            model_kwargs[\"pixel_values\"] = origin_pixel_values\n",
    "            model_kwargs[\"image_grid_thw\"] = origin_image_grid_thw\n",
    "\n",
    "            if streamer is not None:\n",
    "                streamer.put(next_tokens.cpu())\n",
    "\n",
    "            unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n",
    "            this_peer_finished = unfinished_sequences.max() == 0\n",
    "            cur_len += 1\n",
    "\n",
    "            # This is needed to properly delete outputs.logits which may be very large for first iteration\n",
    "            # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\n",
    "            del outputs\n",
    "\n",
    "        if streamer is not None:\n",
    "            streamer.end()\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if self.config.is_encoder_decoder:\n",
    "                return GenerateEncoderDecoderOutput(\n",
    "                    sequences=input_ids,\n",
    "                    scores=scores,\n",
    "                    logits=raw_logits,\n",
    "                    encoder_attentions=encoder_attentions,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    decoder_attentions=decoder_attentions,\n",
    "                    cross_attentions=cross_attentions,\n",
    "                    decoder_hidden_states=decoder_hidden_states,\n",
    "                    past_key_values=model_kwargs.get(\"past_key_values\"),\n",
    "                )\n",
    "            else:\n",
    "                return GenerateDecoderOnlyOutput(\n",
    "                    sequences=input_ids,\n",
    "                    scores=scores,\n",
    "                    logits=raw_logits,\n",
    "                    attentions=decoder_attentions,\n",
    "                    hidden_states=decoder_hidden_states,\n",
    "                    past_key_values=model_kwargs.get(\"past_key_values\"),\n",
    "                )\n",
    "        else:\n",
    "            return input_ids\n",
    "\n",
    "\n",
    "# Subclassing the model, if necessary\n",
    "class CustomQwen2VLForConditionalGeneration(Qwen2VLForConditionalGeneration, CustomGenerationMixin):\n",
    "    def __init__(self, config):\n",
    "        # Call the parent class's constructor to initialize all basic components\n",
    "        super().__init__(config)\n",
    "        # Custom initialization logic\n",
    "        self.image_inputs = None  # To hold image inputs\n",
    "        self.processor = None \n",
    "        self.image_processor = None \n",
    "        self.image_token = 151655  # Assuming this is a defined constant\n",
    "        self.vision_start_token = 151652\n",
    "        self.vision_end_token = 151653\n",
    "    \n",
    "    \n",
    "    def set_image_inputs(self, image_inputs):\n",
    "        \"\"\"\n",
    "        Method to set image inputs outside of the generation call.\n",
    "        This can be called independently before calling generate.\n",
    "        \"\"\"\n",
    "        self.image_inputs = image_inputs\n",
    "\n",
    "\n",
    "    def set_processor(self, processor):\n",
    "        \"\"\"\n",
    "        Method to set image inputs outside of the generation call.\n",
    "        This can be called independently before calling generate.\n",
    "        \"\"\"\n",
    "        self.processor = processor\n",
    "        self.image_processor = processor.image_processor\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        input_ids,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        cache_position=None,\n",
    "        position_ids=None,\n",
    "        use_cache=True,\n",
    "        pixel_values=None,\n",
    "        pixel_values_videos=None,\n",
    "        image_grid_thw=None,\n",
    "        video_grid_thw=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        assert self.image_inputs is not None, \"Image inputs must be provided.\"\n",
    "        origin_input_ids = input_ids\n",
    "        origin_pixel_values = pixel_values\n",
    "        origin_image_grid_thw = image_grid_thw\n",
    "\n",
    "        # Custom handling to check for the special token </IMG>\n",
    "        # Assuming self.tokenizer is available and configured.\n",
    "        if input_ids[:,-1]==151658:\n",
    "            coor_start_ind = torch.where(input_ids==151648)[1].tolist()[-1]\n",
    "            decoded_texts = self.processor.batch_decode(input_ids[:, coor_start_ind:], skip_special_tokens=False)\n",
    "            # Check if any decoded text contains the special token </IMG>\n",
    "            for decoded_text in decoded_texts:\n",
    "                # Retrieve coordinates and index\n",
    "                match = re.search(r\"<\\|box_start\\|\\>\\((\\d+),(\\d+)\\),\\((\\d+),(\\d+)\\)<\\|box_end\\|\\><IMG>(\\d+)</IMG>\", decoded_text)\n",
    "                if match:\n",
    "                    x1, y1, x2, y2, img_index = map(int, match.groups())\n",
    "\n",
    "                    # Validate image index\n",
    "                    if 0 <= img_index < len(self.image_inputs):\n",
    "                        original_image = self.image_inputs[img_index]\n",
    "                        try:\n",
    "                            # Validate and normalize coordinates\n",
    "                            if all(0 <= coord <= 1000 for coord in (x1, y1, x2, y2)) and x1 < x2 and y1 < y2:\n",
    "                                width, height = original_image.size\n",
    "                                x1, y1, x2, y2 = [\n",
    "                                    int(coord / 1000 * (width if i % 2 == 0 else height))\n",
    "                                    for i, coord in enumerate((x1, y1, x2, y2))\n",
    "                                ]\n",
    "                                # Crop the image\n",
    "                                cropped_image = original_image.crop((x1, y1, x2, y2))\n",
    "                            else:\n",
    "                                raise ValueError(f\"Invalid coordinates ({x1}, {y1}, {x2}, {y2})\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in processing image with index {img_index}: {e}\")\n",
    "                            cropped_image = original_image  # Fallback to original image\n",
    "\n",
    "                        # Process cropped image\n",
    "                        new_image_inputs = self.image_processor(images=cropped_image, return_tensors='pt')\n",
    "                        new_pixel_values = torch.tensor(new_image_inputs[\"pixel_values\"], device=input_ids.device)\n",
    "                        new_image_grid_thw = torch.tensor(new_image_inputs[\"image_grid_thw\"], device=input_ids.device)\n",
    "                    else:\n",
    "                        print(f\"Image index {img_index} out of range for {len(self.image_inputs)} images.\")\n",
    "\n",
    "                    # Append vision tokens\n",
    "                    num_image_tokens = new_image_grid_thw.prod() // 4\n",
    "                    vision_token_ids = [self.vision_start_token] + [self.image_token] * num_image_tokens + [self.vision_end_token]\n",
    "                    vision_token_ids_tensor = torch.tensor(vision_token_ids, device=input_ids.device)\n",
    "\n",
    "                    input_ids = torch.cat([input_ids, vision_token_ids_tensor.unsqueeze(0)], dim=1)  # Assuming batch size of 1\n",
    "                    origin_input_ids = input_ids\n",
    "\n",
    "                    cache_position = torch.arange(origin_input_ids.shape[1], device=origin_input_ids.device)\n",
    "\n",
    "                    # Extend the attention mask\n",
    "                    if attention_mask is not None:\n",
    "                        # attention_mask = torch.ones((1, origin_input_ids.shape[1]), device=origin_input_ids.device)\n",
    "                        attention_mask = attention_mask.new_ones((1, origin_input_ids.shape[1]))\n",
    "\n",
    "                    # # Reset past_key_values\n",
    "                    past_key_values = DynamicCache()\n",
    "\n",
    "                    # Combine with original pixel values if needed\n",
    "                    origin_pixel_values = torch.cat([origin_pixel_values, new_pixel_values], dim=0)\n",
    "                    origin_image_grid_thw = torch.cat([origin_image_grid_thw, new_image_grid_thw], dim=0)\n",
    "\n",
    "\n",
    "        # The rest of your prepare_inputs_for_generation logic\n",
    "        model_inputs = super().prepare_inputs_for_generation(\n",
    "            origin_input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            cache_position=cache_position,\n",
    "            position_ids=position_ids,\n",
    "            use_cache=use_cache,\n",
    "            pixel_values=origin_pixel_values,  # use the new pixel values\n",
    "            pixel_values_videos=pixel_values_videos,\n",
    "            image_grid_thw=origin_image_grid_thw,  # use the new grid thw\n",
    "            video_grid_thw=video_grid_thw,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return model_inputs, origin_input_ids, origin_pixel_values, origin_image_grid_thw\n",
    "\n",
    "model_path = \"/local/path/to/model\"\n",
    "model = CustomQwen2VLForConditionalGeneration.from_pretrained(model_path, torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\", device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "def count_model_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Count and print the number of parameters\n",
    "num_parameters = count_model_parameters(model)\n",
    "print(f\"Number of model parameters: {num_parameters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"/mnt/workspace/xiaoxi/code/data/stone/xh_40-img0.png\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"/mnt/workspace/xiaoxi/code/data/stone/xh_40-img1.png\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"/mnt/workspace/xiaoxi/code/data/stone/xh_40-img2.png\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Answer the following multiple-choice question: if there is wind or some external force implemented on the object, which one is most likely to collapse? Options:(A) image 1 (B) image 2 (C) image 3\",\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178684/3488793575.py:335: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_pixel_values = torch.tensor(new_image_inputs[\"pixel_values\"], device=input_ids.device)\n",
      "/tmp/ipykernel_178684/3488793575.py:336: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_image_grid_thw = torch.tensor(new_image_inputs[\"image_grid_thw\"], device=input_ids.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In image 1, the balanced stones (408,135),(672,792)0 are placed on water, which adds an element of instability due to potential movement or disturbance by wind. Image 2 shows stones (198,334),(765,765)1 stacked firmly on a stable surface. Image 3 has a large stone (180,90),(730,530)2 balanced on a wooden stump (210,420),(850,990)2. The wind or external force would more easily disrupt the water and thus the stones in image 1, making it more likely to collapse. Therefore, the answer is A.']\n",
      "['In image 1, the balanced stones <|box_start|>(408,135),(672,792)<|box_end|><IMG>0</IMG><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|> are placed on water, which adds an element of instability due to potential movement or disturbance by wind. Image 2 shows stones <|box_start|>(198,334),(765,765)<|box_end|><IMG>1</IMG><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|> stacked firmly on a stable surface. Image 3 has a large stone <|box_start|>(180,90),(730,530)<|box_end|><IMG>2</IMG><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|> balanced on a wooden stump <|box_start|>(210,420),(850,990)<|box_end|><IMG>2</IMG><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>. The wind or external force would more easily disrupt the water and thus the stones in image 1, making it more likely to collapse. Therefore, the answer is A.<|im_end|>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "# Set image inputs separately\n",
    "model.set_image_inputs(image_inputs)\n",
    "model.set_processor(processor)\n",
    "\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, \n",
    "                                max_new_tokens=8192,\n",
    "                                temperature=0.8,  # Increased from 0.7\n",
    "                                top_k=50,         # Added top_k sampling\n",
    "                                top_p=0.95,       # Added nucleus sampling\n",
    "                                do_sample=True ,   # Enable sampling\n",
    "                                repetition_penalty=1.05)\n",
    "\n",
    "# generated_ids = model.generate(**inputs, max_new_tokens=8192)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(output_text)\n",
    "output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "print(output_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
